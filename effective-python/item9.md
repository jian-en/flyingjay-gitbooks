# 使用生成式（generator）来处理大量的数据

我们掌握的列表的超强威力后，总想着把很多东西都往列表里面塞，比如一个大文件，那直接变成readlines(), 一条条处理；比如从网络上抓取下来的格式化数据，那先全抓下来，放在列表中，循环整个列表，一项一项处理；或者从数据库中读取数据，先用一个SQL语句一次性读取到本机，然后逐条处理。

这样的想法没错，但是如果：文件几个G, 网络数据上万条，数据库数据上亿条，尽管我们的机器资源十分充足，但也经不起这样的折腾。因此，一次性抓取大量数据，然后逐条处理的行为是很危险的。Python中提供了一个很优雅的方式来解决这样的问题，就是传说中的生成器(generator)。

生成器是什么呢？它有点像列表或者宽泛来说是**迭代器**（之后有机会我们再聊迭代器），只不过多了一个关键性的关键词：**yield**。

## 传说中的yield

有一篇文章写的很好：地址如下：http://intermediatepythonista.com/python-generators

一个简单的生成器函数是这样的：

```python
def fib(max):
    a, b = 0, 1
    while a < max:
        yield a
        a, b = b, a + b
```

逐条生成斐波那契数，上条函数运行规则描述起来很简单。

1. a,b 赋值
2. 当a 小于某个固定值，也就是传过来的参数max时，执行下面的循环
3. 返回a这个数，**同时在这条语句冻结住**，可以理解为python保留了这里执行的所有上下文信息，在外层函数获取了此时a的值。

直到next()方法再一次开始调用，又从这个点开始恢复，返回下一个a值。我们可以形象地理解为：

听一个音频，yield就是暂停键，下一次按下播放开关时，还是从上一次停下来的地方继续播放。

## 如何使用

外层使用时，一般会用函数生成一个generator对象。

```python
fib_5 = fib(5)

next(fib_5)  # > 0
next(fib_5)  # > 1
next(fib_5)  # > 1
next(fib_5)  # > 2
next(fib_5)  # > 3
next(fib_5)  # > 报错啦：StopIteration
```

这些特性和迭代器很像，到最后一个值就会抛出越界的异常。

还有一种更加快捷的使用方法，被称为**生成器推导式**，和列表推导式类似，只不过此时用的是小括号。

```python
value = [len(x) for x in open('log.txt')]  # 直接返回这个文件中每一行的字符数，这样会把所有的文件都读进来

value = (len(x) for x in open('log.txt'))  # 返回的是一个generator对象，当我要取下一个时，next(value)，才会把下一行读取到内存中
```

当然它也可以被直接应用在循环中：

```python
for i in fib(10):
    print i
```

此时它的行为会表现的像列表一样，只不过是一条条读取，而不是一下读出来。

## 想法
这绝对是一个很厉害的杀手锏，前一段时间还在思考，数据库中如此多的数据如果处理是比较高效的。有了生成器，我们可以把数据切成小片，优化内存的利用。
